# ğŸ“Š Qualidade de Dados com PySpark: De "Dados Limpos" para "Dados ConfiÃ¡veis"

Este repositÃ³rio foi desenvolvido para a disciplina **Data Collection** do MBA em Engenharia de Dados, focando em **Qualidade de Dados (Data Quality)** - o processo fundamental para garantir dados confiÃ¡veis e decisÃµes baseadas em evidÃªncias.

## ğŸ¯ Objetivos da Aula

Demonstrar na prÃ¡tica as 6 dimensÃµes de qualidade de dados atravÃ©s de implementaÃ§Ãµes PySpark:

- **Completude**: IdentificaÃ§Ã£o e tratamento de valores nulos/vazios
- **Validade**: ValidaÃ§Ã£o de formatos e regras de negÃ³cio
- **Unicidade**: DetecÃ§Ã£o e remoÃ§Ã£o de duplicatas
- **ConsistÃªncia**: VerificaÃ§Ã£o de relacionamentos entre campos
- **AcurÃ¡cia**: DetecÃ§Ã£o de anomalias e outliers
- **Tempestividade**: AnÃ¡lise de freshness dos dados

## ğŸ“š ConteÃºdo ProgramÃ¡tico

### ğŸ“– **Material TeÃ³rico**
- **[aula.md](aula.md)**: Fundamentos de qualidade de dados, dimensÃµes e KQIs
- **PrincÃ­pio GIGO**: Garbage In, Garbage Out
- **Impacto Financeiro**: $3.1T de perdas anuais globais
- **Framework das 6 DimensÃµes**: Estrutura completa de avaliaÃ§Ã£o

### ğŸ’» **ImplementaÃ§Ã£o PrÃ¡tica**
- **qualidade_dados_pyspark.ipynb**: Notebook com implementaÃ§Ãµes PySpark
- **Data Profiling**: AnÃ¡lise exploratÃ³ria automatizada
- **KQIs Dashboard**: MÃ©tricas de qualidade em tempo real
- **Data Cleansing**: CorreÃ§Ãµes automatizadas

### ğŸ¯ **ExercÃ­cio PrÃ¡tico**
- **[exercicio.md](exercicio.md)**: Desafio prÃ¡tico completo
- **[clientes_vendas.csv](clientes_vendas.csv)**: Dataset com problemas intencionais
- **CenÃ¡rio TechCommerce**: Caso real de e-commerce

## ğŸ—ï¸ Arquitetura do Ambiente

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Jupyter Lab   â”‚    â”‚   Apache Spark   â”‚    â”‚   Data Quality  â”‚
â”‚   (Port 8888)   â”‚â—„â”€â”€â–ºâ”‚   + PySpark      â”‚â—„â”€â”€â–ºâ”‚   Framework     â”‚
â”‚                 â”‚    â”‚   (Port 4040)    â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Docker Network    â”‚
                    â”‚  (172.16.240.0/24)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‚ Estrutura do Projeto

```
aulaQualidadeMBA/
â”œâ”€â”€ ğŸ“ notebooks/                    # Notebooks da aula
â”‚   â””â”€â”€ qualidade_dados_pyspark.ipynb  # ImplementaÃ§Ã£o completa das 6 dimensÃµes
â”œâ”€â”€ ğŸ“ data/                         # Datasets
â”‚   â””â”€â”€ clientes_vendas.csv          # Dataset com problemas de qualidade
â”œâ”€â”€ ğŸ“– aula.md                       # Material teÃ³rico completo
â”œâ”€â”€ ğŸ“‹ exercicio.md                  # ExercÃ­cio prÃ¡tico avaliativo
â”œâ”€â”€ ğŸ³ docker-compose.yml            # OrquestraÃ§Ã£o dos serviÃ§os
â”œâ”€â”€ ğŸ³ Dockerfile                    # Imagem PySpark customizada
â””â”€â”€ ğŸ“– README.md                     # Este arquivo
```

## ğŸš€ Setup e ExecuÃ§Ã£o

### PrÃ©-requisitos
- [Docker](https://docs.docker.com/get-docker/) 20.10+
- [Docker Compose](https://docs.docker.com/compose/install/) 2.0+
- 8GB RAM disponÃ­vel
- 10GB espaÃ§o em disco

### ğŸ”¥ Executando na Maquina Local ( Docker Descktop )

1. **Clone o repositÃ³rio**:
   ```bash
   git clone https://github.com/AleTavares/data-quality-spark-lab.git
   cd data-quality-spark-lab
   ```

2. **Inicie o ambiente**:
   ```bash
   docker-compose up -d --build
   ```

3. **Acesse o Jupyter Lab**:
   ```
   http://localhost:8888
   Token: tavares1234
   ```

4. **Acesse o Spark UI** (opcional):
   ```
   http://localhost:4040
   ```

### ğŸ›‘ Parar o ambiente
```bash
docker-compose down
```

## â˜ï¸ Executando no GitHub Codespaces

O GitHub Codespaces oferece um ambiente de desenvolvimento completo na nuvem, ideal para executar este laboratÃ³rio sem necessidade de instalaÃ§Ã£o local.

### ğŸš€ **InÃ­cio RÃ¡pido no Codespaces**

1. **Abra o repositÃ³rio no GitHub**:
   ```
   https://github.com/AleTavares/data-quality-spark-lab
   ```

2. **Crie um novo Codespace**:
   - Clique no botÃ£o verde **"Code"**
   - Selecione a aba **"Codespaces"**
   - Clique em **"Create codespace on main"**

3. **Aguarde a inicializaÃ§Ã£o** (2-3 minutos):
   - O ambiente serÃ¡ configurado automaticamente
   - Docker jÃ¡ estarÃ¡ disponÃ­vel

### ğŸ³ **Executando Docker no Codespace**

4. **Inicie o ambiente PySpark**:
   ```bash
   # No terminal do Codespace
   docker-compose up -d --build
   ```

5. **Aguarde a construÃ§Ã£o** (primeira execuÃ§Ã£o ~5-10 min):
   ```bash
   # Monitore o progresso
   docker-compose logs -f pyspark-quality
   ```

6. **Acesse o Jupyter Lab**:
   - O Codespace detectarÃ¡ automaticamente a porta 8888
   - Clique na notificaÃ§Ã£o **"Open in Browser"**
   - Ou acesse via **"PORTS"** tab â†’ porta 8888
   - **Token**: `tavares1234`

### ğŸ”§ **ConfiguraÃ§Ãµes EspecÃ­ficas do Codespace**

**Recursos Recomendados**:
- **Machine type**: 4-core (8GB RAM) ou superior
- **Storage**: PadrÃ£o (32GB) Ã© suficiente

**Portas Expostas**:
- **8888**: Jupyter Lab (principal)
- **4040**: Spark UI (monitoramento)

**Comandos Ãšteis**:
```bash
# Verificar status dos containers
docker ps

# Ver logs em tempo real
docker-compose logs -f

# Parar o ambiente
docker-compose down

# Limpar recursos (se necessÃ¡rio)
docker system prune -f
```

### âš ï¸ **LimitaÃ§Ãµes do Codespace**
- **Timeout**: Codespaces gratuitos param apÃ³s 30min de inatividade
- **Recursos**: Limitados conforme plano GitHub
- **PersistÃªncia**: Dados persistem enquanto o Codespace existir

### ğŸ’¡ **Dicas de Uso**
- Mantenha o Codespace ativo durante a aula
- FaÃ§a commits regulares para salvar o progresso
- Use `docker-compose down` antes de parar o Codespace

---

## ğŸ“ Roteiro de Estudos

### **Parte 1: Fundamentos TeÃ³ricos**
1. Leia `aula.md` - Material teÃ³rico completo
   - Entenda o impacto da mÃ¡ qualidade ($3.1T anuais)
   - Aprenda o princÃ­pio GIGO (Garbage In, Garbage Out)
   - ConheÃ§a as 6 dimensÃµes de qualidade
   
### **Parte 2: ImplementaÃ§Ã£o PrÃ¡tica**
2. Execute `qualidade_dados_pyspark.ipynb`
   - **Completude**: AnÃ¡lise de valores nulos
   - **Validade**: ValidaÃ§Ã£o de formatos (email, CPF, telefone)
   - **Unicidade**: DetecÃ§Ã£o de duplicatas
   - **ConsistÃªncia**: VerificaÃ§Ã£o entre campos relacionados
   - **AcurÃ¡cia**: DetecÃ§Ã£o de anomalias e outliers
   - **Tempestividade**: AnÃ¡lise de freshness

### **Parte 3: KQIs e Dashboard**
3. Implemente mÃ©tricas de qualidade
   - Calcule KQIs (Key Quality Indicators)
   - Crie dashboard visual de monitoramento
   - Gere relatÃ³rios executivos

### **Parte 4: Data Cleansing**
4. Aplique correÃ§Ãµes automatizadas
   - Remova duplicatas
   - Padronize formatos
   - Filtre registros problemÃ¡ticos

### **Parte 5: ExercÃ­cio Avaliativo**
5. Complete o `exercicio.md`
   - CenÃ¡rio TechCommerce (e-commerce)
   - Dataset `clientes_vendas.csv` com problemas reais
   - ImplementaÃ§Ã£o completa do framework de qualidade

## ğŸ”§ Tecnologias Utilizadas

| Tecnologia | VersÃ£o | PropÃ³sito |
|------------|--------|-----------|
| **Apache Spark** | 3.3.0 | Engine de processamento distribuÃ­do |
| **PySpark** | 3.3.0 | API Python para Spark |
| **Python** | 3.11 | Linguagem principal |
| **Jupyter Lab** | Latest | Ambiente de desenvolvimento |
| **Docker** | 20.10+ | ContainerizaÃ§Ã£o |
| **Pandas** | Latest | AnÃ¡lise de dados complementar |

## ğŸ¯ Framework de Qualidade de Dados

### âœ… **As 6 DimensÃµes Implementadas**

| DimensÃ£o | MÃ©trica | **ImplementaÃ§Ã£o PySpark** |
|----------|---------|---------------------------|
| **Completude** | % campos preenchidos | `count(when(col().isNotNull()))` |
| **Validade** | % formatos corretos | `regexp_match()` + validaÃ§Ãµes |
| **Unicidade** | % registros Ãºnicos | `dropDuplicates()` + anÃ¡lise |
| **ConsistÃªncia** | % relacionamentos vÃ¡lidos | ValidaÃ§Ãµes entre campos |
| **AcurÃ¡cia** | % dados corretos | DetecÃ§Ã£o de outliers/anomalias |
| **Tempestividade** | Freshness dos dados | AnÃ¡lise temporal |

### ğŸš€ **Casos de Uso Empresariais**
- **Business Intelligence**: RelatÃ³rios confiÃ¡veis e precisos
- **Machine Learning**: Datasets limpos para modelos robustos
- **Compliance**: Atendimento a regulamentaÃ§Ãµes (LGPD)
- **OperaÃ§Ãµes**: ReduÃ§Ã£o de custos operacionais
- **Tomada de DecisÃ£o**: DecisÃµes baseadas em dados confiÃ¡veis

## ğŸ¤ ContribuiÃ§Ãµes

Este material foi desenvolvido para fins educacionais. SugestÃµes e melhorias sÃ£o bem-vindas atravÃ©s de issues e pull requests.

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## ğŸ“ Suporte

Para dÃºvidas sobre o conteÃºdo da aula:
- Abra uma [issue](https://github.com/AleTavares/dataqualitySpark/issues)
- Entre em contato durante a aula

---

**ğŸ“ MBA Engenharia de Dados - Data Collection**  
*Transformando dados em valor atravÃ©s da Qualidade de Dados*